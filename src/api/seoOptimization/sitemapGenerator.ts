// Sitemap Generator
// Generates sitemap.xml and robots.txt

import {
	SitemapData,
	SitemapUrl,
	SITEMAP_PRIORITIES,
	SITEMAP_CHANGEFREQ,
} from '@/types/seoPackage';
import { SEOOptimizedPage } from '@/types/seoPackage';

// ============================================================================
// PRIORITY AND CHANGEFREQ
// ============================================================================

/**
 * Get sitemap priority for a page type
 */
export function getPriority(pageType: string): number {
	return SITEMAP_PRIORITIES[pageType] ?? 0.5;
}

/**
 * Get change frequency for a page type
 */
export function getChangeFreq(pageType: string): SitemapUrl['changefreq'] {
	return SITEMAP_CHANGEFREQ[pageType] ?? 'monthly';
}

// ============================================================================
// XML GENERATION
// ============================================================================

/**
 * Generate sitemap XML content
 */
export function generateSitemapXML(urls: SitemapUrl[]): string {
	const urlEntries = urls
		.map(
			(url) => `  <url>
    <loc>${escapeXml(url.loc)}</loc>
    <lastmod>${url.lastmod}</lastmod>
    <changefreq>${url.changefreq}</changefreq>
    <priority>${url.priority.toFixed(1)}</priority>
  </url>`,
		)
		.join('\n');

	return `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
${urlEntries}
</urlset>`;
}

/**
 * Escape special XML characters
 */
function escapeXml(str: string): string {
	return str
		.replace(/&/g, '&amp;')
		.replace(/</g, '&lt;')
		.replace(/>/g, '&gt;')
		.replace(/"/g, '&quot;')
		.replace(/'/g, '&apos;');
}

// ============================================================================
// ROBOTS.TXT GENERATION
// ============================================================================

/**
 * Generate robots.txt content
 */
export function generateRobotsTxt(baseUrl: string): string {
	return `# Robots.txt for ${baseUrl}
# Generated by SEO Optimization Node

User-agent: *
Allow: /

# Disallow admin and private pages
Disallow: /admin/
Disallow: /private/
Disallow: /api/
Disallow: /_next/
Disallow: /thank-you
Disallow: /confirmation

# Crawl delay
Crawl-delay: 1

# Sitemap location
Sitemap: ${baseUrl}/sitemap.xml

# Special instructions for specific bots
User-agent: GPTBot
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: CCBot
Disallow: /
`;
}

// ============================================================================
// MAIN GENERATOR
// ============================================================================

/**
 * Generate complete sitemap data from optimized pages
 */
export function generateSitemap(
	pages: SEOOptimizedPage[],
	baseUrl: string,
): SitemapData {
	const today = new Date().toISOString().split('T')[0];

	const urls: SitemapUrl[] = pages
		.filter((page) => {
			// Exclude pages that shouldn't be indexed
			return (
				page.meta.robots.includes('index') &&
				!page.url.includes('thank-you') &&
				!page.url.includes('confirmation') &&
				!page.url.includes('error')
			);
		})
		.map((page) => ({
			loc: `${baseUrl}${page.url}`,
			lastmod: today,
			changefreq: getChangeFreq(page.type),
			priority: getPriority(page.type),
		}))
		.sort((a, b) => b.priority - a.priority); // Sort by priority descending

	const xml = generateSitemapXML(urls);

	return {
		xml,
		urls,
	};
}

/**
 * Generate sitemap index for large sites (> 50,000 URLs)
 */
export function generateSitemapIndex(
	sitemaps: Array<{ loc: string; lastmod: string }>,
): string {
	const entries = sitemaps
		.map(
			(sitemap) => `  <sitemap>
    <loc>${escapeXml(sitemap.loc)}</loc>
    <lastmod>${sitemap.lastmod}</lastmod>
  </sitemap>`,
		)
		.join('\n');

	return `<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
${entries}
</sitemapindex>`;
}

/**
 * Split pages into multiple sitemaps if needed
 */
export function splitSitemaps(
	pages: SEOOptimizedPage[],
	baseUrl: string,
	maxUrlsPerSitemap: number = 50000,
): SitemapData[] {
	const sitemaps: SitemapData[] = [];
	const today = new Date().toISOString().split('T')[0];

	// Filter indexable pages
	const indexablePages = pages.filter((page) =>
		page.meta.robots.includes('index'),
	);

	// Split into chunks
	for (let i = 0; i < indexablePages.length; i += maxUrlsPerSitemap) {
		const chunk = indexablePages.slice(i, i + maxUrlsPerSitemap);
		const urls: SitemapUrl[] = chunk.map((page) => ({
			loc: `${baseUrl}${page.url}`,
			lastmod: today,
			changefreq: getChangeFreq(page.type),
			priority: getPriority(page.type),
		}));

		sitemaps.push({
			xml: generateSitemapXML(urls),
			urls,
		});
	}

	return sitemaps;
}
